<!doctype html>
<html lang="en">
<head>
  <meta name="google-site-verification" content="H36R6DdSFsXbK_EYJ0d2btAgPpL-J_p1KIjnaaAEnkE" />
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/rndimg.jpg">


   <title>A ConvMixer architecture: ðŸ¤·</title> 

</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
  
    <div class="sidebar-about">

      <h1><a class="sidebar-about" href="/">Jacob Shkrob</a></h1>
      <p class="sidebar-about">Mathematics PhD Student @ NYU</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">About</a>
      <a class="sidebar-nav-item " href="/research/">Research</a>
      <a class="sidebar-nav-item " href="/notes/">Notes</a>
      <a class="sidebar-nav-item " href="/other/">Other</a>

    </nav>

    <div class="sidebar-foot">
      <p>&copy; Jacob Shkrob, modified: January 10, 2023. <br> Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language. </a></p>
    </div>


  </div>
</div>
<div class="content container">

<!-- Content appended here -->


<div class="franklin-content">
   <h1 class="page-title"> A ConvMixer architecture: ðŸ¤· </h1> 
   <span class="page-date"> December 2010 </span> 
</div>
<div class="franklin-content">
<p>In computer vision, <strong>Residual Networks</strong> &#40;ResNets&#41; are an important and successful architecture; beating the performance of deep ResNets on image classification tasks like ImageNet remains a noticeable achievement. In natural language processing, <strong>Transformers</strong> play the same role and are now widely used and studied, especially through Bert-like architectures. </p>
<p>Residual networks are mostly based on <em>convolutions</em>; that is, they transform images by applying local filters, and they combine these convolutions in various way. On the other hand, Transformers are based on <em>attention mechanisms</em>: they consider words in sentences as elementary units and learn which ones are the most important. Using attention mechanisms in vision successfully led to <strong>Vision Transformers</strong>, but it is not clear why they perform well; in trying to understand this, a few purely-convolutional architectures emerged, who reached very good performances while at the same time being really simpler than ResNets.  </p>
<p>The goal of this post is to show one of these architectures, <em>ConvMixer</em>, implemented with Julia&#39;s <a href="https://fluxml.ai/Flux.jl/stable/">Flux</a> machine learning library. </p>
<p><img src="/posts/img/logo.png" alt="flux" /></p>
<p><hr /> </p>
<div class="franklin-toc"><ol><li><a href="#overview_vision_transformers_and_the_role_of_patches">Overview:Â Vision Transformers and the role of patches</a></li><li><a href="#build_the_mixer">Build the mixer</a></li><li><a href="#train_the_mixer">Train the mixer </a></li><li><a href="#im_not_a_code_golfer">I&#39;m not a code golfer</a></li><li><a href="#julias_flux_library">Julia&#39;s Flux library</a></li><li><a href="#references_the_paper_title_competition">References: the Paper Title Competition</a></li></ol></div>
<hr />
<h2 id="overview_vision_transformers_and_the_role_of_patches"><a href="#overview_vision_transformers_and_the_role_of_patches" class="header-anchor">Overview:Â Vision Transformers and the role of patches</a></h2>
<p>Since 2017, it was widely discussed in natural language processing whether <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">attention is all you need</a>. The term <strong>attention</strong> refers to learning the relations between different bits of the input. For text processing, the bits are words or sentences;Â attention mechanisms &#40;and also self-attention&#41; learn this, leading to what is called Transformers. </p>
<p>This idea was adapted to vision tasks. There are no words in pictures, but one can still split any picture in <strong>patches</strong> and treat them as words in a text, feeding them to Transformer architectures, hence the name <em>Vision Transformers</em> &#40;ViT&#41;. It works, and indeed <a href="https://arxiv.org/pdf/2010.11929.pdf">an image is worth 16x16 words</a>. But it is not completely clear if the success of these methods comes from these attention mechanisms: in other words, <a href="https://arxiv.org/pdf/2105.02723.pdf">do you even need attention?</a></p>
<p>It seems that splitting images in patches, then stacking classical networks applied to these patches is enough for very high performances. Indeed, another recent model, <code>ConvMixer</code>, also suggested that <a href="https://openreview.net/pdf?id&#61;TVHS5Y4dNvM">patches are all you need</a>, and that it is rather the patch representation of pictures which is responsible for this gain in performance. </p>
<p>The <code>ConvMixer</code> is really simple: it first splits the image in different patches, and then feeds it to a chain of convolutional networks alternatively applied channel-wise or space-wise. No recurrence or self-attention here, and yet this model reaches excellent performances&#33;</p>
<h2 id="build_the_mixer"><a href="#build_the_mixer" class="header-anchor">Build the mixer</a></h2>
<p>Let&#39;s build the architecture. All the basic blocks &#40;convolutions, nonlinearities, batch normalizations, mean pooling and dense layers&#41; are natively available in Flux, we only have to assemble them. The input of our architecture will be a batch of \(B\) images of size \(32 \times 32\) with three &#40;RGB&#41; channels, hence the input dimension is </p>
\[(32, 32, 3, B).\]
<p>The architecture of ConvMixer can be summarized in the following picture, taken from the paper: </p>
<p><img src="/posts/img/convmixerarchi.png" alt="" /></p>
<p>Let&#39;s decompose it block by block.</p>
<h3 id="chain"><a href="#chain" class="header-anchor">Chain</a></h3>
<p>In Flux, the basic way to compose layers is <code>Chain</code>, the equivalent of Torch&#39;s <code>nn.Sequential</code>. There&#39;s nothing more to say. </p>
<h3 id="patch_embedding_as_a_convolution"><a href="#patch_embedding_as_a_convolution" class="header-anchor">Patch embedding as a convolution</a></h3>
<p>The main point of the ConvMixer architecture is that it begins by splitting an image in patches of size <code>&#40;p, p&#41;</code>. This is done using the convolutional layer and the <code>stride</code> argument. Let us take a \((p,p)\) convolution. If <code>stride&#61;1</code>, the convolution is applied around every pixel in the image. If <code>stride &#61; 2</code>, it is applied only on one over two pixels, etc. With <code>stride&#61;p</code>, the windows on which the convolution is applied are all disjoint and adjacent, thus covering the image in \((p,p)\) patches. This is why the first layer of ConvMixer is </p>
<pre><code class="language-julia">Conv&#40;&#40;p, p&#41;, 3&#61;&gt;H, gelu; stride&#61;p&#41;.</code></pre>
<p>In Flux, the third argument of a convolution, if specified, is an activation function applied pointwise<sup id="fnref:1"><a href="#fndef:1" class="fnref">[1]</a></sup>. Here, we take the <a href="https://paperswithcode.com/method/gelu">Gaussian error linear unit</a>, Gelu. Additionnaly, we transform the 3 initial RGB channels in \(H\) &#40;like <em>hidden</em>&#41; dimensions. After this operation, the batch has dimension</p>
\[(32/p, 32/p, H, B)\]
<h3 id="batchnorm"><a href="#batchnorm" class="header-anchor">Batchnorm</a></h3>
<p>In ConvMixer as in many networks, each layer is followed by a batch normalization:Â after being filtered, the arrays representing images in a batch are centered and reduced along each dimension. There are two learnable parameters &#40;mean and std&#41; for each dimension. In Flux, we simply call <code>BatchNorm&#40;H&#41;</code>. </p>
<h3 id="residual_networks"><a href="#residual_networks" class="header-anchor">Residual networks</a></h3>
<p>The next ingredient for the ConvMixer architecture is the <em>residual connection</em>: instead of filtering \(x\) to \(f(x)\) and feeding \(f(x)\) to the next layer, we feed \(x+f(x)\) to the next layer. In Flux, this is done with <code>SkipConnection&#40;layer, &#43;&#41;</code> where <code>layer</code> is any chain of layers. Note that we are not forced to use addition <code>&#43;</code> to perform the connection. We could multiply, concatenate or anything else.  </p>
<p>We use these SkipConnections with layers composed of channel-wise convolutions: here the <code>groups</code> argument tells us that the convolution is applied indepedently on each channel.  </p>
<pre><code class="language-julia">SkipConnection&#40;
    Chain&#40;
        Conv&#40;&#40;kernel,kernel&#41;, H&#61;&gt;H, gelu; pad&#61;SamePad&#40;&#41;, groups&#61;H&#41;,
        BatchNorm&#40;H&#41;&#41;, 
    &#43;
&#41;</code></pre>
<p>This residual layer is followed by a purely pixel-wise convolutional layer, and this whole operation is repeated <code>depth</code> times. </p>
<h3 id="last_layers"><a href="#last_layers" class="header-anchor">Last layers</a></h3>
<p>At the end of the last convolutional layer, we have a batch of dimension \((32/p, 32/p, H, B)\). It is time to reduce dimensions:Â we average over each channel with <code>AdaptativeMeanPool&#40;&#40;1,1&#41;&#41;</code> to obtain a \((1,1,H, B)\) batch, then we apply a dense layer &#40;which is only applied to flattened arrays, hence the <code>flatten</code> layer right before&#41;:Â </p>
<pre><code class="language-julia">Chain&#40;AdaptiveMeanPool&#40;&#40;1,1&#41;&#41;, flatten, Dense&#40;H,N_classes&#41;&#41;</code></pre>
<h3 id="the_final_implementation"><a href="#the_final_implementation" class="header-anchor">The final implementation</a></h3>
<pre><code class="language-julia">using Flux

function ConvMixer&#40;in_channels, H, k, patch, depth, N_classes&#41;
    return Chain&#40;
            Conv&#40;&#40;patch, patch&#41;, in_channels&#61;&gt;H, gelu; stride&#61;patch&#41;,
            BatchNorm&#40;H&#41;,
            &#91;
                Chain&#40;
                    SkipConnection&#40;
                        Chain&#40;
                            Conv&#40;&#40;k,k&#41;,H&#61;&gt;H,gelu; pad&#61;SamePad&#40;&#41;, groups&#61;H&#41;, 
                            BatchNorm&#40;H&#41;
                        &#41;, 
                    &#43;&#41;,
                    Chain&#40;Conv&#40;&#40;1,1&#41;, H&#61;&gt;H, gelu&#41;, BatchNorm&#40;H&#41;&#41;
                &#41; 
                for i in 1:depth
            &#93;...,
            AdaptiveMeanPool&#40;&#40;1,1&#41;&#41;,
            flatten,
            Dense&#40;H,N_classes&#41;
        &#41;
    return f
end</code></pre>
<p>Let&#39;s compute the number of parameters:Â the patch-splitting convolution has \(3 \times H\times p^2\) parameters. The BatchNorm layer has \(2H\) parameters. Each of the <code>depth</code> layers has \(H\times k^2 + 2H + H^2 + 2H\) parameters &#40;conv, batchnorm, conv, batchnorm&#41;. The last layer is affine, hence it has \(H \times N + N\) parameters. Overall we have</p>
\[\mathscr{P}(k,p,H,N, \text{depth}) = 3 H p^2 + 2H + \text{depth}(Hk^2 + 4H + H^2) + NH + N \]
<p>learnable parameters in this architecture<sup id="fnref:2"><a href="#fndef:2" class="fnref">[2]</a></sup>. </p>
<h2 id="train_the_mixer"><a href="#train_the_mixer" class="header-anchor">Train the mixer </a></h2>
<h3 id="the_cifar10_dataset"><a href="#the_cifar10_dataset" class="header-anchor">The Cifar10 Dataset</a></h3>
<p>Training large networks on the reference ImageNet dataset is resource-consuming. I&#39;m sticking to the smaller dataset CIFAR10. </p>
<pre><code class="language-julia">using MLDatasets
using Flux:onehotbatch, Dataloader

function get_CIFAR_data&#40;batchsize; idxs &#61; nothing&#41;
    &quot;&quot;&quot;
        idxs&#61;nothing gives the full dataset.
        otherwise only the 1:idxs elements of the train set are given.
    &quot;&quot;&quot;
    ENV&#91;&quot;DATADEPS_ALWAYS_ACCEPT&quot;&#93; &#61; &quot;true&quot; 

    if idxs&#61;&#61;nothing
        xtrain, ytrain &#61; MLDatasets.CIFAR.traindata&#40;Float32&#41;
        xtest, ytest &#61; MLDatasets.CIFAR.testdata&#40;Float32&#41;
	else
        xtrain, ytrain &#61; MLDatasets.CIFAR.traindata&#40;Float32, 1:idxs&#41;
        xtest, ytest &#61; MLDatasets.CIFAR.testdata&#40;Float32, 1:Int&#40;idxs/10&#41;&#41;
    end

    # Reshape Data to comply to Julia&#39;s convention:
    #&#40;width, height, channels, batch_size&#41;
    xtrain &#61; reshape&#40;xtrain, &#40;32,32,3,:&#41;&#41;
    xtest &#61; reshape&#40;xtest, &#40;32,32,3,:&#41;&#41;
    ytrain, ytest &#61; onehotbatch&#40;ytrain, 0:9&#41;, onehotbatch&#40;ytest, 0:9&#41;

    train_loader &#61; DataLoader&#40;&#40;xtrain, ytrain&#41;, batchsize, shuffle&#61;true&#41;
    test_loader &#61; DataLoader&#40;&#40;xtest, ytest&#41;, batchsize&#41;

    return train_loader, test_loader
end</code></pre>
<p>Flux&#39;s <code>Dataloader</code> splits the data in batches of the given size, possibly shuffled, and returns an iterable object whose elements are \((x,y)\) where \(x\) is a batch and \(y\) the corresponding labels.  </p>
<p>Also, we shouldn&#39;t train our model on raw CIFAR10;Â we should augment it using classical procedures &#40;random modifications, mixups and so on&#41;. I&#39;ll do this next time using <code>Augmentor.jl</code>.</p>
<h3 id="gpu_support"><a href="#gpu_support" class="header-anchor">GPU support</a></h3>
<p>Flux uses the CUDA toolbox: the <code>device&#40;T&#41;</code> method takes any object <code>T</code> and puts it on <code>device</code>. If you have a batch of images and labels <code>x,y</code> drawn from your dataloader, you put it on the gpu using <code>x &#61; gpu&#40;x&#41;</code> and back on the cpu with <code>x &#61; cpu&#40;x&#41;</code>.</p>
<h3 id="cross-entropy_loss_and_classification_accuracy"><a href="#cross-entropy_loss_and_classification_accuracy" class="header-anchor">Cross-entropy loss and classification accuracy</a></h3>
<p>Since we deal with a classification task &#40;there are 10 classes on CIFAR&#41;, we&#39;ll train the network using the logit cross entropy. Just for the sake of writing maths, let&#39;s recall how it works. With \(C = 10\) classes, our architecture is fed an image \(x_i\), and outputs the probability \(\hat{p}_{i, c}\) that this image belongs to class \(c\). The ground truth would be \(p_{i,c}=0\) for all the classes \(c\) except for the <em>real</em> class \(c\) of image \(x_i\), for which \(p_{i,c}=1\). The cross-entropy &#40;<code>Flux.logitcrossentropy</code>&#41; measures this discrepancy in the following way:</p>
\[ L(\hat{p}_i, p_i) = - \sum_{c=1}^C \log(\hat{p}_{i,c})p_{i,c}.\]
<p>We&#39;ll also keep track of the accuracy of the model: since we want to predict classes, and not only probabilities, we predict the class of an image to be the one with the highest predicted probability \(\hat{p}_{i,c}\). That&#39;s what the <code>onecold</code> function from Flux does. The proportion of correct predictions is the accuracy. </p>
<pre><code class="language-julia">using Flux:onecold, logitcrossentropy

function â„“&#40;dataloader, model, device&#41;
    &quot;&quot;&quot;batch-wise loss and accuracy&quot;&quot;&quot;
    
    n &#61; 0
    cross_entropy &#61; 0.0f0
    accuracy &#61; 0.0f0

    for &#40;x,y&#41; in dataloader
        x,y &#61; x |&gt; device, y |&gt; device
        z &#61; model&#40;x&#41;        
        cross_entropy &#43;&#61; logitcrossentropy&#40;z, y, agg&#61;sum&#41;
        accuracy &#43;&#61; sum&#40;onecold&#40;z&#41;.&#61;&#61;onecold&#40;y&#41;&#41;
        n &#43;&#61; size&#40;x&#41;&#91;end&#93;
    end
    cross_entropy / n, accuracy / n
end</code></pre>
<h3 id="training_loop"><a href="#training_loop" class="header-anchor">Training loop</a></h3>
<p>Let&#39;s put everything together. </p>
<pre><code class="language-julia">using Flux:Optimiser
using BSON:@save

function train&#40;n_epochs&#61;200, Î·&#61;3e-4, device&#61;gpu&#41;

    train_loader, test_loader &#61; get_data&#40;128&#41;
    patch_size &#61; 2
    kernel_size &#61; 7
    dim &#61; 128
    depth &#61; 8

    #for saving the losses and accuracy
    train_save &#61; zeros&#40;n_epochs, 2&#41;
    test_save &#61; zeros&#40;n_epochs, 2&#41;

    model &#61; ConvMixer&#40;3, kernel_size, patch_size, dim, depth, 10&#41; |&gt; device

    ps &#61; params&#40;model&#41;
    opt &#61; Optimiser&#40;
            WeightDecay&#40;1f-3&#41;, 
            ClipNorm&#40;1.0&#41;,
            ADAM&#40;Î·&#41;
            &#41;

    for epoch in 1:n_epochs
        for &#40;x,y&#41; in train_loader
            x,y &#61; x|&gt;device, y|&gt;device
            gr &#61; gradient&#40;&#40;&#41;-&gt;Flux.logitcrossentropy&#40;model&#40;x&#41;, y, agg&#61;sum&#41;, ps&#41;
            Flux.Optimise.update&#33;&#40;opt, ps, gr&#41;
        end

        #logging
        train_loss, train_acc &#61; â„“&#40;train_loader, model, device&#41; |&gt; cpu
        test_loss, test_acc &#61; â„“&#40;test_loader, model, device&#41; |&gt; cpu
        train_save&#91;epoch,:&#93; &#61; &#91;train_loss, train_acc&#93;
        test_save&#91;epoch,:&#93; &#61; &#91;test_loss, test_acc&#93;

        if epoch&#37;5&#61;&#61;0
            @info &quot;t&#61;&#36;epoch : Train loss &#61; &#36;train_loss | Test acc. &#61; &#36;test_acc.&quot;
        end

    end

    model &#61; model |&gt; cpu
    @save &quot;save/model.bson&quot; model 
    @save &quot;save/losses.bson&quot; train_save test_save
end</code></pre>
<p>Some comments:</p>
<ol>
<li><p>Patches are small &#40;\(2 \times 2\)&#41;. For small datasets like CIFAR, we don&#39;t need large patches and indeed the best results are obtained with \(p=1\). However, for experiments on larger datasets like ImageNet where pictures are \(224\times 224\), patches of sizes 7,8 or 9 perform well &#40;all this according to the ConvMixer paper&#41;.</p>
</li>
</ol>
<ol start="2">
<li><p>The optimiser includes weight decay <em>and</em> gradient clipping with naive parameters. </p>
</li>
<li><p>The parameters were mostly taken from the ConvMixer paper; I chose them small enough so that the training of this model roughly took one afternoon on a Tesla P100.</p>
</li>
</ol>
<ol start="4">
<li><p>This network has less than 200k parameters, which is quite tiny. </p>
</li>
</ol>
<h3 id="training_curves"><a href="#training_curves" class="header-anchor">Training curves</a></h3>
<p>Most of the training time was spent overfitting with few improvement in the generalization error. </p>
<p><img src="/posts/img/fig.png" alt="" /></p>
<p>After 200 training epochs on batches of size 128, I got a labelling accuracy of 74&#37;. <strong>That&#39;s bad</strong>, but I underoptimised nearly everything:Â no augmentations on the dataset, no optimiser parameter scheduler, and a pretty small network.  For reference, the best performances on CIFAR are above 95&#37;;Â the best ConvMixer performance seems close to 97&#37; &#40;with 1.3 million parameters&#41;. </p>
<h2 id="im_not_a_code_golfer"><a href="#im_not_a_code_golfer" class="header-anchor">I&#39;m not a code golfer</a></h2>
<p>The authors of the ConvMixer paper argue that their architecture is as powerful as ResNets or ConViT high-performance models without any hyperparameter tuning, but the conceptual complexity of the model is considerably simpler: in fact, it is simplistic enough to fit in one tweet, ie less than 280 characters when implemented with the PyTorch framework. </p>
<p><img src="/posts/img/convmixer_singletweet.png " alt="a single tweet" /></p>
<p>This implies a little bit of <a href="https://en.wikipedia.org/wiki/Code_golf">code golf</a> cheating. In Julia, you can fit everything in a single tweet, <code>using Flux</code> included, in 272 characters without even needing to golf your functions names: </p>
<p><img src="/posts/img/carbon.png " alt="even better" /></p>
<p>The code is here &#40;with spaces&#41;:</p>
<blockquote>
<code>ConvMixer&#40;k,p,h,N&#41; &#61; Chain&#40;Conv&#40;&#40;p,p&#41;, 3&#61;&gt;h, gelu;stride&#61;p&#41;, BatchNorm&#40;h&#41;, &#91;Chain&#40;SkipConnection&#40;Chain&#40;Conv&#40;&#40;k,k&#41;,h&#61;&gt;h,gelu;pad&#61;SamePad&#40;&#41;,groups&#61;1&#41;, Batchnorm&#40;h&#41;&#41;,&#43;&#41;, Chain&#40;Conv&#40;&#40;1,1&#41;,h&#61;&gt;h,gelu&#41;, BatchNorm&#40;h&#41;&#41;&#41; for i in 1:D&#93;..., AdaptiveMeanPool&#40;&#40;1,1&#41;&#41;, flatten, Dense&#40;h,N&#41;&#41;</code>
</blockquote>
<h2 id="julias_flux_library"><a href="#julias_flux_library" class="header-anchor">Julia&#39;s Flux library</a></h2>
<p>The main goal of this post was to give a serious try to Julia&#39;s Flux library for deep learning. In short, it works. All the needed functionalities are here: the basic building blocks of deep vison architectures &#40;convolutions, residuals, batchnorms, nonlinearities, LSTM, RNN, etc.&#41; - I do not know if it&#39;s the same for NLP tools of GNNs. Autodiff works well and you can easily write your own gradients using the <code>@adjoint</code> macro. </p>
<p>Using a single GPUÂ for MLÂ experiments is just as easy as in any other framework. GPU computing works well with CUDA.jl<sup id="fnref:3"><a href="#fndef:3" class="fnref">[3]</a></sup>. I stumbled on bugs but the <a href="https://juliagpu.org/">JuliaGPU team</a> and <a href="https://github.com/maleadt">@maleadt &#40;Tim Besard&#41;</a> solved them at godspeed. </p>
<p>Last but not least, Flux benefits from the high flexibility and readability of the Julia language.   </p>
<p>In conclusion, there are many frameworks for doing deepl learning research right now:Â PyTorch, TensorFlow and satellites &#40;MXNet, Sonnet&#41;, Keras, JAX, Flux, Matlab... I&#39;m not a fan of communities wars, but it&#39;s fair to say that overall, PyTorch won:Â it&#39;s more flexible, it has the largest community, it&#39;s more robust and stable. But just as Torch or TF or Keras, <strong>Flux is also excellent</strong>. It has everything we need. It&#39;s sufficiently powerful, well-documented and flexible to be used in a day-to-day basis for Machine Learning research, and most importantly, <strong>to focus on content rather than on implementation</strong>. All hail Flux&#33; </p>
<h2 id="references_the_paper_title_competition"><a href="#references_the_paper_title_competition" class="header-anchor">References: the Paper Title Competition</a></h2>
<p><a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is all you need</a> was published 4 years ago and has 33k citations. </p>
<p><a href="https://arxiv.org/pdf/2010.11929.pdf">An image is worth 16x16 words</a> was written in 2020. </p>
<p>The first paper emphasizing the role of patches seems to be <a href="https://arxiv.org/pdf/2105.02723.pdf">Do you even need attention?</a>. </p>
<p>The original ConvMixer paper is <a href="https://openreview.net/pdf?id&#61;TVHS5Y4dNvM">Patches are all you need?</a> - the style is quite unconventional for a conference paper ðŸ¤·. </p>
<p>Most of my implementation of the training loop is inspired by the excellent <a href="https://github.com/FluxML/model-zoo">Flux Model Zoo</a>, a collection of simple implementations of classical deep learning architectures. </p>
<h3 id="notes"><a href="#notes" class="header-anchor">Notes</a></h3>
<p><sup id="fnref:1"><a href="#fndef:1" class="fnref">[1]</a></sup> In Torch, you would use <code>nn.Sequential&#40;nn.Conv2d&#40;...&#41;, nn.GeLU&#40;&#41;&#41;</code>. </p>
<p><sup id="fnref:2"><a href="#fndef:2" class="fnref">[2]</a></sup> The Convmixer paper mentions a slightly different formula; I&#39;m under the impression that their Batchnorm layers have \(3\) learnable parameters per channel. Where am I wrong?</p>
<p><sup id="fnref:3"><a href="#fndef:3" class="fnref">[3]</a></sup> Unfortunately, people having a non-NVIDIA GPU will have a hard time using it, and not only with JuliaGPU. Nvidia definitely took the high ground in the GPU game for machine learning.</p>
<!-- 
  Page footer is in the sidebar. 
 --></div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
